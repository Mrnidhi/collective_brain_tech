# Collective Brain of Tech Students

## Overview
A real-time dashboard to analyze and visualize what students and developers are learning, using, and struggling with across GitHub, Stack Overflow, and Reddit.

---

## ğŸ“¦ Project Structure
```
/project-root
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ trend_analysis/
â”œâ”€â”€ fetch_sources/
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ config.toml
â”œâ”€â”€ .env.example      # (Do not upload actual API keys!)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ fetched_data.json (optional, as sample)
â”œâ”€â”€ trends.json       (optional, as sample)
â””â”€â”€ setup.sh          (optional for HuggingFace)
```

---

## ğŸš€ How to Run Locally
1. **Clone the repo**
2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```
3. **Set up your .env file:**
   - Copy `.env.example` to `.env` and fill in your API keys.
4. **Fetch live data:**
   ```bash
   python run_fetch.py
   ```
5. **Generate trends:**
   ```bash
   python trend_analysis/run_trends.py
   ```
6. **Launch the dashboard:**
   ```bash
   streamlit run dashboard/app.py
   ```

---

## â˜ï¸ Deploying to Streamlit Cloud
1. **Push this project to a public GitHub repo.**
2. **Go to [Streamlit Cloud](https://streamlit.io/cloud) and connect your repo.**
3. **Set the main file as:**
   ```
   dashboard/app.py
   ```
4. **Add your secrets (.env values) via Streamlit Cloud Secrets Manager.**
5. **[Optional] Add sample_data/fetched_data.json and trends.json for demo mode.**

---

## ğŸ§ª Demo Mode (Optional)
- If no API keys are available, the app can use sample data from `sample_data/`.
- To try demo mode, copy files from `sample_data/` to the project root.

---

## ğŸ¤– HuggingFace Spaces (Optional)
- Add a `setup.sh` if you need custom install steps.
- Set the app entry point to `dashboard/app.py`.

---

## ğŸ” Secrets & API Keys
- **Never commit your real .env file!**
- Use `.env.example` as a template.
- For deployment, add secrets via the platform's secrets manager.

## Phase 1 â€“ Real-time Data Ingestion

This project fetches, analyzes, and normalizes what students and developers are learning, using, and struggling with across GitHub, Stack Overflow, and Reddit.

### Features
- Fetches commit messages, repo names, and languages from GitHub Public Events API
- Fetches recent questions, tags, and scores from Stack Overflow
- Fetches post titles, upvotes, and timestamps from Reddit subreddits: r/learnprogramming, r/cscareerquestions, r/AskProgramming
- Normalizes all data into a unified format and stores it in a local JSON file

---

## ğŸ“ File Structure
```
collective_brain/
â”‚
â”œâ”€â”€ fetch_sources/
â”‚   â”œâ”€â”€ github_fetcher.py
â”‚   â”œâ”€â”€ so_fetcher.py
â”‚   â”œâ”€â”€ reddit_fetcher.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ run_fetch.py        # main orchestrator
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env                # (for Reddit API keys)
â””â”€â”€ README.md
```

---

## âš¡ Setup Instructions

1. **Clone the repository**
2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```
3. **Set up Reddit API keys:**
   - Create a `.env` file in the root directory:
     ```ini
     REDDIT_CLIENT_ID=your_id_here
     REDDIT_SECRET=your_secret_here
     ```
4. **Run the main script:**
   ```bash
   python run_fetch.py
   ```
   This will fetch and save data to `fetched_data.json`.

---

## ğŸ§ª Example Output
```json
{
  "source": "github",
  "timestamp": "2025-06-19T14:22:10Z",
  "text": "fix: handle null pointer bug in parser",
  "tags": ["bug"],
  "meta": {
    "lang": "python",
    "repo": "user/repo",
    "url": "https://github.com/user/repo"
  }
}
```

---

## Next Steps
- Topic extraction and trending (Phase 2)
- Streamlit dashboard (Phase 3)
